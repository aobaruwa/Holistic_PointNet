#!/bin/bash
#SBATCH --partition=preempt ### Partition (like a queue in PBS)
#SBATCH --gpus=1             ### Number of gpus
#SBATCH --mem-per-cpu=128G
#SBATCH --cpus-per-task=1
#SBATCH --constraint='volta'
#SBATCH --job-name=full_est.   ### Job Name
#SBATCH --output=/home/abaruwa/datascience/landmark_mine/src/landmark/Point_Net/pnet.out         ### File in which to store job output
#SBATCH --error=/home/abaruwa/datascience/landmark_mine/src/landmark/Point_Net/pnet.err          ### File in which to store job error messages
#SBATCH --time=01-00:00:00           ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --ntasks-per-node=1         ### Number of tasks to be launched per Node
#SBATCH --account=datascience       ### Account used for job submission

python train_point_net.py 50 4 100 102400 "/projects/datascience/shared/DATA/train.npy" "/projects/datascience/shared/DATA/val.npy"  "/home/abaruwa/datascience/landmark_mine/src/landmark/Point_Net/logs"